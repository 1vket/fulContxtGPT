
model:
  conv_embd: False
  conv_block: False
  n_kernel: 5
  n_stride: 1
  n_pad: 4
  n_embd: 768
  n_head: 12
  n_layer: 12
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  resid_pdrop: 0.1
  vocab_size: 54
  block_size: 1024
  pad_idx: 0
  sos_idx: 1
  eos_idx: 2
  max_length: 512

train:
  max_epochs: 1
  batch_size: 32
  learning_rate: 0.00003
  betas: [0.9, 0.95]
  grad_norm_clip: 1.0
  weight_decay: 0.1
  lr_decay: False
  warmup_tokens: 16000000
  final_tokens: 1300000000

  ckpt_path: "logg/728emb12L12nt/"
  num_workers: 0

