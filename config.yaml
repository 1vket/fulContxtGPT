
model:
  n_embd: 256
  n_head: 8
  n_layer: 3
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  resid_pdrop: 0.1
  vocab_size: 48
  block_size: 512
  pad_idx: 0

train:
  max_epochs: 50
  batch_size: 64
  learning_rate: 0.0003
  betas: [0.9, 0.95]
  grad_norm_clip: 1.0
  weight_decay: 0.1
  lr_decay: True
  warmup_tokens: 16000000
  final_tokens: 130000000

  ckpt_path: "bestmodel"
  num_workers: 0

